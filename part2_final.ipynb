{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:57:45.282830Z","iopub.status.busy":"2024-05-07T15:57:45.282301Z","iopub.status.idle":"2024-05-07T15:57:52.130476Z","shell.execute_reply":"2024-05-07T15:57:52.129476Z","shell.execute_reply.started":"2024-05-07T15:57:45.282804Z"},"trusted":true},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","import torch\n","import numpy as np\n","import torch.optim as optim\n","import os\n","import h5py\n","import matplotlib.pyplot as plt\n","from PIL import Image\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","from torch.utils.data import Dataset, DataLoader\n","dir = '/kaggle/input/a2data/'\n","out_dir = '/kaggle/working/'\n","batch_size = 64\n"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:57:52.132618Z","iopub.status.busy":"2024-05-07T15:57:52.132226Z","iopub.status.idle":"2024-05-07T15:58:06.827278Z","shell.execute_reply":"2024-05-07T15:58:06.826162Z","shell.execute_reply.started":"2024-05-07T15:57:52.132593Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting einops\n","  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n","Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m619.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\u001b[36m0:00:01\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.8.0\n"]}],"source":["!pip install einops"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:06.829408Z","iopub.status.busy":"2024-05-07T15:58:06.829056Z","iopub.status.idle":"2024-05-07T15:58:07.035546Z","shell.execute_reply":"2024-05-07T15:58:07.034741Z","shell.execute_reply.started":"2024-05-07T15:58:06.829376Z"},"trusted":true},"outputs":[],"source":["\n","from einops import rearrange\n","\n","\n","class VectorQuantizer(nn.Module):\n","    \"\"\"\n","    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n","    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n","    \"\"\"\n","\n","    # NOTE: due to a bug the beta term was applied to the wrong term. for\n","    # backwards compatibility we use the buggy version by default, but you can\n","    # specify legacy=False to fix it.\n","    def __init__(\n","        self,\n","        n_e,\n","        e_dim,\n","        beta,\n","        remap=None,\n","        unknown_index=\"random\",\n","        sane_index_shape=False,\n","        legacy=True,\n","    ):\n","        super().__init__()\n","        self.n_e = n_e\n","        self.e_dim = e_dim\n","        self.beta = beta\n","        self.legacy = legacy\n","\n","        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n","        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n","\n","        self.remap = remap\n","        if self.remap is not None:\n","            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n","            self.re_embed = self.used.shape[0]\n","            self.unknown_index = unknown_index  # \"random\" or \"extra\" or integer\n","            if self.unknown_index == \"extra\":\n","                self.unknown_index = self.re_embed\n","                self.re_embed = self.re_embed + 1\n","            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n","                  f\"Using {self.unknown_index} for unknown indices.\")\n","        else:\n","            self.re_embed = n_e\n","\n","        self.sane_index_shape = sane_index_shape\n","\n","    def remap_to_used(self, inds):\n","        ishape = inds.shape\n","        assert len(ishape) > 1\n","        inds = inds.reshape(ishape[0], -1)\n","        used = self.used.to(inds)\n","        match = (inds[:, :, None] == used[None, None, ...]).long()\n","        new = match.argmax(-1)\n","        unknown = match.sum(2) < 1\n","        if self.unknown_index == \"random\":\n","            new[unknown] = torch.randint(\n","                0, self.re_embed,\n","                size=new[unknown].shape).to(device=new.device)\n","        else:\n","            new[unknown] = self.unknown_index\n","        return new.reshape(ishape)\n","\n","    def unmap_to_all(self, inds):\n","        ishape = inds.shape\n","        assert len(ishape) > 1\n","        inds = inds.reshape(ishape[0], -1)\n","        used = self.used.to(inds)\n","        if self.re_embed > self.used.shape[0]:  # extra token\n","            inds[inds >= self.used.shape[0]] = 0  # simply set to zero\n","        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)\n","        return back.reshape(ishape)\n","\n","    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n","        assert temp is None or temp == 1.0, \"Only for interface compatible with Gumbel\"\n","        assert rescale_logits == False, \"Only for interface compatible with Gumbel\"\n","        assert return_logits == False, \"Only for interface compatible with Gumbel\"\n","        # reshape z -> (batch, height, width, channel) and flatten\n","        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n","        z_flattened = z.view(-1, self.e_dim)\n","        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n","\n","        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n","            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n","            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n","\n","        min_encoding_indices = torch.argmin(d, dim=1)\n","        z_q = self.embedding(min_encoding_indices).view(z.shape)\n","        perplexity = None\n","        min_encodings = None\n","\n","        # compute loss for embedding\n","        if not self.legacy:\n","            loss = self.beta * torch.mean((z_q.detach()-z)**2) + \\\n","                   torch.mean((z_q - z.detach()) ** 2)\n","        else:\n","            loss = torch.mean((z_q.detach()-z)**2) + self.beta * \\\n","                   torch.mean((z_q - z.detach()) ** 2)\n","\n","        # preserve gradients\n","        z_q = z + (z_q - z).detach()\n","\n","        # reshape back to match original input shape\n","        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n","\n","        if self.remap is not None:\n","            min_encoding_indices = min_encoding_indices.reshape(\n","                z.shape[0], -1)  # add batch axis\n","            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n","            min_encoding_indices = min_encoding_indices.reshape(-1,\n","                                                                1)  # flatten\n","\n","        if self.sane_index_shape:\n","            min_encoding_indices = min_encoding_indices.reshape(\n","                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n","\n","        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n","\n","    def get_codebook_entry(self, indices, shape):\n","        # shape specifying (batch, height, width, channel)\n","        if self.remap is not None:\n","            indices = indices.reshape(shape[0], -1)  # add batch axis\n","            indices = self.unmap_to_all(indices)\n","            indices = indices.reshape(-1)  # flatten again\n","\n","        # get quantized latent vectors\n","        z_q = self.embedding(indices)\n","\n","        if shape is not None:\n","            z_q = z_q.view(shape)\n","            # reshape back to match original input shape\n","            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n","\n","        return z_q\n","\n","def nonlinearity(x):\n","    # swish\n","    return x * torch.sigmoid(x)\n","\n","\n","def Normalize(in_channels, num_groups=32):\n","    return torch.nn.GroupNorm(\n","        num_groups=num_groups, num_channels=in_channels, eps=1e-6, affine=True)\n","\n","\n","class Upsample(nn.Module):\n","\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            self.conv = torch.nn.Conv2d(\n","                in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        x = torch.nn.functional.interpolate(\n","            x, scale_factor=2.0, mode=\"nearest\")\n","        if self.with_conv:\n","            x = self.conv(x)\n","        return x\n","\n","\n","class Downsample(nn.Module):\n","\n","    def __init__(self, in_channels, with_conv):\n","        super().__init__()\n","        self.with_conv = with_conv\n","        if self.with_conv:\n","            # no asymmetric padding in torch conv, must do it ourselves\n","            self.conv = torch.nn.Conv2d(\n","                in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n","\n","    def forward(self, x):\n","        if self.with_conv:\n","            pad = (0, 1, 0, 1)\n","            x = torch.nn.functional.pad(x, pad, mode=\"constant\", value=0)\n","            x = self.conv(x)\n","        else:\n","            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n","        return x\n","\n","\n","class ResnetBlock(nn.Module):\n","\n","    def __init__(\n","        self,\n","        *,\n","        in_channels,\n","        out_channels=None,\n","        conv_shortcut=False,\n","        dropout,\n","    ):\n","        super().__init__()\n","        self.in_channels = in_channels\n","        out_channels = in_channels if out_channels is None else out_channels\n","        self.out_channels = out_channels\n","        self.use_conv_shortcut = conv_shortcut\n","\n","        self.norm1 = Normalize(in_channels)\n","        self.conv1 = torch.nn.Conv2d(\n","            in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        self.norm2 = Normalize(out_channels)\n","        self.dropout = torch.nn.Dropout(dropout)\n","        self.conv2 = torch.nn.Conv2d(\n","            out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                self.conv_shortcut = torch.nn.Conv2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size=3,\n","                    stride=1,\n","                    padding=1)\n","            else:\n","                self.nin_shortcut = torch.nn.Conv2d(\n","                    in_channels,\n","                    out_channels,\n","                    kernel_size=1,\n","                    stride=1,\n","                    padding=0)\n","\n","    def forward(self, x):\n","        h = x\n","        h = self.norm1(h)\n","        h = nonlinearity(h)\n","        h = self.conv1(h)\n","\n","        h = self.norm2(h)\n","        h = nonlinearity(h)\n","        h = self.dropout(h)\n","        h = self.conv2(h)\n","\n","        if self.in_channels != self.out_channels:\n","            if self.use_conv_shortcut:\n","                x = self.conv_shortcut(x)\n","            else:\n","                x = self.nin_shortcut(x)\n","\n","        return x + h\n","    \n","class PositionalEmbeddings(nn.Module):\n","    def __init__(self, H, W, hid_dim=64):\n","        super(PositionalEmbeddings, self).__init__()\n","        self.H = H\n","        self.W = W\n","        self.hid_dim = hid_dim\n","        self.project = nn.Linear(4, hid_dim)\n","    \n","    def construct_grid(self, H, W):\n","        x = torch.linspace(0, 1, W).unsqueeze(0).repeat(H, 1)\n","        y = torch.linspace(0, 1, H).unsqueeze(1).repeat(1, W)\n","        return torch.stack([x, 1-x, y, 1-y], dim=2)    # (H, W, 4)\n","\n","\n","    def forward(self, inputs):\n","        grid = self.construct_grid(self.H, self.W).to(device)  # (H, W, 4)\n","        grid = self.project(grid)\n","        return inputs + grid.unsqueeze(0).expand(inputs.size(0), self.H, self.W, self.hid_dim)\n","\n","\n","class AttnBlock(nn.Module):\n","\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.in_channels = in_channels\n","\n","        self.norm = Normalize(in_channels)\n","        self.q = torch.nn.Conv2d(\n","            in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n","        self.k = torch.nn.Conv2d(\n","            in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n","        self.v = torch.nn.Conv2d(\n","            in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n","        self.proj_out = torch.nn.Conv2d(\n","            in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n","\n","    def forward(self, x):\n","        h_ = x\n","        h_ = self.norm(h_)\n","        q = self.q(h_)\n","        k = self.k(h_)\n","        v = self.v(h_)\n","\n","        # compute attention\n","        b, c, h, w = q.shape\n","        q = q.reshape(b, c, h * w)\n","        q = q.permute(0, 2, 1)  # b,hw,c\n","        k = k.reshape(b, c, h * w)  # b,c,hw\n","        w_ = torch.bmm(q, k)  # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n","        w_ = w_ * (int(c)**(-0.5))\n","        w_ = torch.nn.functional.softmax(w_, dim=2)\n","\n","        # attend to values\n","        v = v.reshape(b, c, h * w)\n","        w_ = w_.permute(0, 2, 1)  # b,hw,hw (first hw of k, second of q)\n","        h_ = torch.bmm(\n","            v, w_)  # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n","        h_ = h_.reshape(b, c, h, w)\n","\n","        h_ = self.proj_out(h_)\n","\n","        return x + h_\n","\n","\n","def make_attn(in_channels, attn_type=\"vanilla\"):\n","    assert attn_type in [\"vanilla\", \"none\"], f'attn_type {attn_type} unknown'\n","    print(f\"making '{attn_type}' attention with {in_channels} in_channels\")\n","    if attn_type == \"vanilla\":\n","        return AttnBlock(in_channels)\n","    elif attn_type == \"none\":\n","        return nn.Identity(in_channels)\n","    else:\n","        raise NotImplementedError(f'attn_type {attn_type} not implemented')\n","\n","\n","class Encoder(nn.Module):\n","\n","    def __init__(\n","        self,\n","        *,\n","        ch,\n","        out_ch,\n","        ch_mult=(1, 2, 4, 8),\n","        num_res_blocks,\n","        attn_resolutions,\n","        dropout=0.0,\n","        resamp_with_conv=True,\n","        in_channels,\n","        resolution,\n","        z_channels,\n","        double_z=False,\n","        attn_type=\"vanilla\",\n","        **ignore_kwargs,\n","    ):\n","        super().__init__()\n","\n","        self.ch = ch\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # downsampling\n","        self.conv_in = torch.nn.Conv2d(\n","            in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n","\n","        curr_res = resolution\n","        in_ch_mult = (1, ) + tuple(ch_mult)\n","        self.in_ch_mult = in_ch_mult\n","        self.down = nn.ModuleList()\n","        for i_level in range(self.num_resolutions):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_in = ch * in_ch_mult[i_level]\n","            block_out = ch * ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks):\n","                block.append(\n","                    ResnetBlock(\n","                        in_channels=block_in,\n","                        out_channels=block_out,\n","                        dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(make_attn(block_in, attn_type=attn_type))\n","            down = nn.Module()\n","            down.block = block\n","            down.attn = attn\n","            if i_level != self.num_resolutions - 1:\n","                down.downsample = Downsample(block_in, resamp_with_conv)\n","                curr_res = curr_res // 2\n","            self.down.append(down)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(\n","            in_channels=block_in, out_channels=block_in, dropout=dropout)\n","        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n","        self.mid.block_2 = ResnetBlock(\n","            in_channels=block_in, out_channels=block_in, dropout=dropout)\n","\n","        # end\n","        self.norm_out = Normalize(block_in)\n","        out_ch = z_channels * 2 if double_z else z_channels\n","        self.conv_out = torch.nn.Conv2d(\n","            block_in, out_ch, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, x):\n","        # downsampling\n","        hs = [self.conv_in(x)]\n","        for i_level in range(self.num_resolutions):\n","            for i_block in range(self.num_res_blocks):\n","                h = self.down[i_level].block[i_block](hs[-1])\n","                if len(self.down[i_level].attn) > 0:\n","                    h = self.down[i_level].attn[i_block](h)\n","                hs.append(h)\n","            if i_level != self.num_resolutions - 1:\n","                hs.append(self.down[i_level].downsample(hs[-1]))\n","\n","        # middle\n","        h = hs[-1]\n","        h = self.mid.block_1(h)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h)\n","\n","        # end\n","        h = self.norm_out(h)\n","        h = nonlinearity(h)\n","        h = self.conv_out(h)\n","        return h\n","\n","\n","class Decoder(nn.Module):\n","\n","    def __init__(\n","        self,\n","        *,\n","        ch,\n","        out_ch,\n","        ch_mult=(1, 2, 4, 8),\n","        num_res_blocks,\n","        attn_resolutions,\n","        dropout=0.0,\n","        resamp_with_conv=True,\n","        in_channels,\n","        resolution,\n","        z_channels,\n","        attn_type=\"vanilla\",\n","        **ignorekwargs,\n","    ):\n","        super().__init__()\n","\n","        self.ch = ch\n","        self.num_resolutions = len(ch_mult)\n","        self.num_res_blocks = num_res_blocks\n","        self.resolution = resolution\n","        self.in_channels = in_channels\n","\n","        # compute in_ch_mult, block_in and curr_res at lowest res\n","        in_ch_mult = (1, ) + tuple(ch_mult)\n","        block_in = ch * ch_mult[self.num_resolutions - 1]\n","        curr_res = resolution // 2**(self.num_resolutions - 1)\n","        self.z_shape = (1, z_channels, curr_res, curr_res)\n","        print(\"Working with z of shape {} = {} dimensions.\".format(\n","            self.z_shape, np.prod(self.z_shape)))\n","\n","        # z to block_in\n","        self.conv_in = torch.nn.Conv2d(\n","            z_channels, block_in, kernel_size=3, stride=1, padding=1)\n","\n","        # middle\n","        self.mid = nn.Module()\n","        self.mid.block_1 = ResnetBlock(\n","            in_channels=block_in, out_channels=block_in, dropout=dropout)\n","        self.mid.attn_1 = make_attn(block_in, attn_type=attn_type)\n","        self.mid.block_2 = ResnetBlock(\n","            in_channels=block_in, out_channels=block_in, dropout=dropout)\n","\n","        # upsampling\n","        self.up = nn.ModuleList()\n","        for i_level in reversed(range(self.num_resolutions)):\n","            block = nn.ModuleList()\n","            attn = nn.ModuleList()\n","            block_out = ch * ch_mult[i_level]\n","            for i_block in range(self.num_res_blocks + 1):\n","                block.append(\n","                    ResnetBlock(\n","                        in_channels=block_in,\n","                        out_channels=block_out,\n","                        dropout=dropout))\n","                block_in = block_out\n","                if curr_res in attn_resolutions:\n","                    attn.append(make_attn(block_in, attn_type=attn_type))\n","            up = nn.Module()\n","            up.block = block\n","            up.attn = attn\n","            if i_level != 0:\n","                up.upsample = Upsample(block_in, resamp_with_conv)\n","                curr_res = curr_res * 2\n","            self.up.insert(0, up)  # prepend to get consistent order\n","\n","        # end\n","        self.norm_out = Normalize(block_in)\n","        self.conv_out = torch.nn.Conv2d(\n","            block_in, out_ch, kernel_size=3, stride=1, padding=1)\n","\n","    def forward(self, z):\n","        # assert z.shape[1:] == self.z_shape[1:]\n","        self.last_z_shape = z.shape\n","\n","        # z to block_in\n","        h = self.conv_in(z)\n","\n","        # middle\n","        h = self.mid.block_1(h)\n","        h = self.mid.attn_1(h)\n","        h = self.mid.block_2(h)\n","\n","        # upsampling\n","        for i_level in reversed(range(self.num_resolutions)):\n","            for i_block in range(self.num_res_blocks + 1):\n","                h = self.up[i_level].block[i_block](h)\n","                if len(self.up[i_level].attn) > 0:\n","                    h = self.up[i_level].attn[i_block](h)\n","            if i_level != 0:\n","                h = self.up[i_level].upsample(h)\n","\n","        h = self.norm_out(h)\n","        h = nonlinearity(h)\n","        h = self.conv_out(h)\n","        return h\n","\n","\n","def temporal_wrapper(func):\n","    \"\"\"A wrapper to make the model compatible with both 4D and 5D inputs.\"\"\"\n","\n","    def f(cls, x):\n","        \"\"\"x is either [B, C, H, W] or [B, T, C, H, W].\"\"\"\n","        B = x.shape[0]\n","        if len(x.shape) == 5:\n","            unflatten = True\n","            x = x.flatten(0, 1)\n","        else:\n","            unflatten = False\n","\n","        outs = func(cls, x)\n","\n","        if unflatten:\n","            if isinstance(outs, tuple):\n","                outs = [o.unflatten(0, (B, -1)) if o.ndim else o for o in outs]\n","                return tuple(outs)\n","            else:\n","                return outs.unflatten(0, (B, -1))\n","        else:\n","            return outs\n","\n","    return f\n","\n","\n","class VAE(torch.nn.Module):\n","    \"\"\"VQ-VAE consisting of Encoder, QuantizationLayer and Decoder.\"\"\"\n","\n","    def __init__(\n","        self,\n","        enc_dec_dict=dict(\n","            resolution=128,\n","            in_channels=3,\n","            z_channels=3,\n","            ch=64,  # base_channel\n","            ch_mult=[1, 2, 4],  # num_down = len(ch_mult)-1\n","            num_res_blocks=2,\n","            attn_resolutions=[],\n","            out_ch=3,\n","            dropout=0.0,\n","        ),\n","        vq_dict=dict(\n","            n_embed=4096,  # vocab_size\n","            embed_dim=3,  # same as `z_channels`\n","            percept_loss_w=1.0,\n","        ),\n","        use_loss=True,\n","    ):\n","        super().__init__()\n","\n","        self.resolution = enc_dec_dict['resolution']\n","        self.embed_dim = vq_dict['embed_dim']\n","        self.n_embed = vq_dict['n_embed']\n","        self.z_ch = enc_dec_dict['z_channels']\n","\n","        self.encoder = Encoder(**enc_dec_dict)\n","        self.decoder = Decoder(**enc_dec_dict)\n","\n","        self.quantize = VectorQuantizer(\n","            self.n_embed,\n","            self.embed_dim,\n","            beta=0.25,\n","            sane_index_shape=True,\n","        )\n","        self.quant_conv = nn.Conv2d(self.z_ch, self.embed_dim, 1)\n","        self.post_quant_conv = nn.Conv2d(self.embed_dim, self.z_ch, 1)\n","\n","        # if use_loss:\n","        #     self.loss = VQLPIPSLoss(percept_loss_w=vq_dict['percept_loss_w'])\n","\n","    @temporal_wrapper\n","    def encode_quantize(self, x):\n","        \"\"\"Encode image to pre-VQ features, then quantize.\"\"\"\n","        h = self.encode(x)  # `embed_dim`\n","        quant, quant_loss, (_, _, quant_idx) = self.quantize(h)\n","        # [B, `embed_dim`, h, w], scalar, [B*h*w]\n","        return quant, quant_loss, quant_idx\n","\n","    @temporal_wrapper\n","    def encode(self, x):\n","        \"\"\"Encode image to pre-VQ features.\"\"\"\n","        # this is the x0 in LDM!\n","        h = self.encoder(x)  # `z_ch`\n","        h = self.quant_conv(h)  # `embed_dim`\n","        return h\n","    \n","    def decode(self, latent):\n","        quant, quant_loss, (_, _, quant_idx) = self.quantize(latent)\n","        image = self._decode(quant)\n","        return image\n","\n","    @temporal_wrapper\n","    def quantize_decode(self, h):\n","        \"\"\"Input pre-VQ features, quantize and decode to reconstruct.\"\"\"\n","        # use this to reconstruct images from LDM's denoised output!\n","        quant, _, _ = self.quantize(h)\n","        dec = self.decode(quant)\n","        return dec\n","\n","    @temporal_wrapper\n","    def _decode(self, quant):\n","        \"\"\"Input already quantized features, do reconstruction.\"\"\"\n","        quant = self.post_quant_conv(quant)  # `z_ch`\n","        dec = self.decoder(quant)\n","        return dec\n","\n","    def forward(self, data_dict):\n","        img = data_dict['img']\n","        quant, quant_loss, token_id = self.encode_quantize(img)\n","        recon = self.decode(quant)\n","        out_dict = {\n","            'recon': recon,\n","            'token_id': token_id,\n","            'quant_loss': quant_loss,\n","        }\n","        return out_dict\n","\n","    def calc_train_loss(self, data_dict, out_dict):\n","        \"\"\"Compute training loss.\"\"\"\n","        img = data_dict['img']\n","        recon = out_dict['recon']\n","        quant_loss = out_dict['quant_loss']\n","\n","        loss_dict = self.loss(quant_loss, img, recon)\n","\n","        return loss_dict\n","\n","    @torch.no_grad()\n","    def calc_eval_loss(self, data_dict, out_dict):\n","        \"\"\"Loss computation in eval.\"\"\"\n","        loss_dict = self.calc_train_loss(data_dict, out_dict)\n","        img = data_dict['img']\n","        recon = out_dict['recon']\n","        loss_dict['recon_mse'] = F.mse_loss(recon, img)\n","        return loss_dict\n","\n","    @property\n","    def dtype(self):\n","        return self.quant_conv.weight.dtype\n","\n","    @property\n","    def device(self):\n","        return self.quant_conv.weight.device\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.039219Z","iopub.status.busy":"2024-05-07T15:58:07.038467Z","iopub.status.idle":"2024-05-07T15:58:07.060735Z","shell.execute_reply":"2024-05-07T15:58:07.059896Z","shell.execute_reply.started":"2024-05-07T15:58:07.039174Z"},"trusted":true},"outputs":[],"source":["class SlotAttention(nn.Module):\n","    def __init__(self, k, d_common=64, n_iter_train=3,n_iter_test=5, d_slot=64, d_inputs=64, hid_dim=128):\n","        super(SlotAttention, self).__init__()\n","        self.k = k\n","        self.d_common = d_common\n","        self.n_iter_train = n_iter_train\n","        self.n_iter_test = n_iter_test\n","        self.d_slot = d_slot\n","        self.d_inputs = d_inputs\n","\n","        self.fc_q = nn.Linear(d_slot, d_common)\n","        self.fc_k = nn.Linear(d_inputs, d_common)\n","        self.fc_v = nn.Linear(d_inputs, d_common)\n","\n","        self.gru = nn.GRUCell(d_common, d_slot)\n","        self.mlp = nn.Sequential(\n","            nn.Linear(d_slot, hid_dim),\n","            nn.ReLU(),\n","            nn.Linear(hid_dim, d_slot)\n","        )\n","\n","        self.softmax = nn.Softmax(dim=2)\n","        self.mu = nn.Parameter(torch.randn(1, 1,d_common))\n","        self.sigma = nn.Parameter(torch.rand(1,1, d_common))\n","\n","    \n","    def forward(self, inputs):\n","        # inputs: (batch_size, n_inputs, d_inputs)\n","        # slots: (batch_size, n_slots, d_slot)\n","        if self.training:\n","            n_iter = self.n_iter_train\n","        else:\n","            n_iter = self.n_iter_train\n","        batch_size, n_inputs, d_inputs = inputs.size()\n","        mu = self.mu.expand(batch_size, self.k, -1).to(device)\n","        sigma = self.sigma.expand(batch_size, self.k, -1)\n","#         sigma = torch.ones(batch_size, self.k, self.d_common).to(device)*0.1\n","#         a = sigma < 0\n","#         if torch.any(a).item():\n","#         print(\"SIGMA: \", sigma)\n","        slots = torch.normal(mu, sigma).to(device)\n","        inputs = nn.LayerNorm(d_inputs).to(device)(inputs)\n","        k = self.fc_k(inputs)               # (batch_size, n_inputs, d_common)\n","        v = self.fc_v(inputs)               # (batch_size, n_inputs, d_common)\n","        masks = 0\n","        for i in range(n_iter):\n","            q = self.fc_q(nn.LayerNorm(self.d_slot).to(device)(slots))                # (batch_size, n_slots, d_common)\n","            attn = torch.bmm(k, q.permute(0, 2, 1)) / np.sqrt(self.d_common)            # (batch_size, n_inputs, n_slots)\n","            attn = self.softmax(attn) +  1e-8                                           # (batch_size, n_inputs, n_slots)\n","            attn = attn / attn.sum(dim=1, keepdim=True)                                 # (batch_size, n_inputs, n_slots)\n","            attn = attn.permute(0,2,1)\n","            updates = torch.einsum('bjd,bij->bid', v, attn)                             # (batch_size, n_slots, d_common)\n","\n","            masks = attn\n","            slots = self.gru(updates.reshape(-1,self.d_common), slots.reshape(-1, self.d_slot)).reshape(batch_size, self.k, self.d_slot)\n","            slots = nn.LayerNorm(self.d_slot).to(device)(slots)\n","            slots = slots + self.mlp(slots)\n","        masks = masks.view(masks.shape[0], masks.shape[1], 128, 128)\n","        return slots, masks\n","\n","class CNNEncoder(nn.Module):\n","    def __init__(self, hid_dim=64):\n","        super(CNNEncoder, self).__init__()\n","        self.conv1 = nn.Conv2d(3, hid_dim, 5, padding=2)                    \n","        self.conv2 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n","        self.conv3 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n","        self.conv4 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n","\n","        self.positionalEmb = PositionalEmbeddings(128, 128, hid_dim)\n","        self.relu = nn.ReLU()\n","\n","        self.fc1 = nn.Linear(hid_dim, hid_dim)\n","        self.fc2 = nn.Linear(hid_dim, hid_dim)  \n","\n","\n","    def forward(self, inputs):\n","        inputs = self.conv1(inputs)\n","        inputs = self.relu(inputs)\n","        inputs = self.conv2(inputs)\n","        inputs = self.relu(inputs)\n","        inputs = self.conv3(inputs)\n","        inputs = self.relu(inputs)\n","        inputs = self.conv4(inputs)\n","        inputs = self.relu(inputs)\n","        \n","        inputs = self.positionalEmb(inputs.permute(0, 2, 3, 1))\n","        inputs = inputs.flatten(1, 2)\n","        inputs = nn.LayerNorm(inputs.size()[1:]).to(device)(inputs) \n","        inputs = self.fc1(inputs)\n","        inputs = self.relu(inputs)\n","        inputs = self.fc2(inputs)\n","        return inputs\n","           "]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.062077Z","iopub.status.busy":"2024-05-07T15:58:07.061762Z","iopub.status.idle":"2024-05-07T15:58:07.077666Z","shell.execute_reply":"2024-05-07T15:58:07.076850Z","shell.execute_reply.started":"2024-05-07T15:58:07.062030Z"},"trusted":true},"outputs":[],"source":["class RESBLOCK(nn.Module):\n","    def __init__(self, in_channels, out_channels, emb_channels, dropout=0.1, type_='nosample'):\n","        super(RESBLOCK, self).__init__()\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.norm1 = nn.GroupNorm(32, in_channels)\n","        self.norm2 = nn.GroupNorm(32, out_channels)\n","        self.type = type_\n","        self.SiLU = nn.SiLU()\n","        if type_ == 'downsample':\n","            self.conv1 = nn.AvgPool2d(kernel_size=2, stride=2)\n","        elif type_ == 'upsample':\n","            # interpolate; this is an irrelevant line\n","            pass\n","#             self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n","        else:\n","            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=1)\n","        self.linear = nn.Linear(emb_channels, out_channels)\n","        self.dropout = nn.Dropout(dropout)\n","\n","        self.conv2 = nn.Conv2d(out_channels,out_channels, kernel_size=3, padding=1, stride=1)\n","    \n","    def forward(self, x, time_embedding):\n","        x1 = self.norm1(x)\n","        x1 = self.SiLU(x1)\n","        if(self.type == 'upsample'):\n","            x1 = F.interpolate(x1, scale_factor=2, mode='bilinear', align_corners = False)\n","        else:\n","            x1 = self.conv1(x1)\n","        b, c, h, w = x1.size()\n","        # dimension of time_embedding = (batch_size, emb_channels)\n","        time_embedding = self.linear(self.SiLU(time_embedding))\n","        time_embedding = time_embedding.unsqueeze(-1).unsqueeze(-1)\n","        x1 = x1 + time_embedding\n","        x = self.norm2(x1)\n","        x = self.SiLU(x)\n","        x = self.dropout(x) # ??\n","        x = self.conv2(x)\n","        x = x + x1\n","        return x\n","\n","\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.079046Z","iopub.status.busy":"2024-05-07T15:58:07.078792Z","iopub.status.idle":"2024-05-07T15:58:07.184329Z","shell.execute_reply":"2024-05-07T15:58:07.183556Z","shell.execute_reply.started":"2024-05-07T15:58:07.079024Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["False tensor([[False, False, False, False],\n","        [False, False, False, False]])\n"]}],"source":["a = torch.tensor([[1,2,4, 1],[1,2,1,2]])\n","b = a<0\n","print(torch.any(b).item(), b)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.185449Z","iopub.status.busy":"2024-05-07T15:58:07.185187Z","iopub.status.idle":"2024-05-07T15:58:07.193874Z","shell.execute_reply":"2024-05-07T15:58:07.192907Z","shell.execute_reply.started":"2024-05-07T15:58:07.185427Z"},"trusted":true},"outputs":[],"source":["class AttnBlock1(nn.Module):\n","    def __init__(self, in_channels, num_heads, slot_dim = 64):\n","        super(AttnBlock1, self).__init__()\n","        self.in_channels = in_channels\n","        self.self_attn = nn.MultiheadAttention(self.in_channels, num_heads, batch_first=True)\n","        self.norm1 = nn.LayerNorm(self.in_channels)\n","        self.norm2 = nn.LayerNorm(self.in_channels)\n","        self.norm3 = nn.LayerNorm(self.in_channels)\n","        self.cross_attn = nn.MultiheadAttention(self.in_channels, num_heads, batch_first=True, kdim = slot_dim, vdim = slot_dim)\n","        self.linear1 = nn.Linear(self.in_channels, 4*self.in_channels)\n","        self.gelu = nn.GELU()\n","        self.linear2 = nn.Linear(4*self.in_channels, self.in_channels)\n","        self.dropout = nn.Dropout(0.1)\n","        \n","    def forward(self, x, c):\n","        x = x + self.norm1(self.self_attn(x, x, x)[0])\n","        x = x + self.norm2(self.cross_attn(x, c, c)[0])\n","        x = x + self.norm3(self.linear2(self.dropout(self.gelu(self.linear1(x)))))\n","        return x\n","        "]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.195460Z","iopub.status.busy":"2024-05-07T15:58:07.195114Z","iopub.status.idle":"2024-05-07T15:58:07.212473Z","shell.execute_reply":"2024-05-07T15:58:07.211799Z","shell.execute_reply.started":"2024-05-07T15:58:07.195430Z"},"trusted":true},"outputs":[],"source":["class AttentionBlock(nn.Module):\n","    def __init__(self, in_channels, x):\n","        super(AttentionBlock, self).__init__()\n","        self.head_dim = 32\n","        self.channels = in_channels\n","        self.iter = x\n","        self.num_heads = in_channels // self.head_dim\n","        self.norm1 = nn.GroupNorm(32, in_channels)\n","        self.onexone = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1)\n","        # self.self_attn = []\n","        # self.cross_attn = []\n","        # self.ffn = []\n","        # for i in range(x):\n","        #     self.self_attn.append(nn.MultiheadAttention(self.channels, self.num_heads, batch_first=True))\n","        #     self.cross_attn.append(nn.MultiheadAttention(self.channels, self.num_heads, batch_first=True))\n","        #     self.ffn.append(nn.Sequential(\n","        #         nn.Linear(self.channels, self.channels),\n","        #         nn.SiLU(),\n","        #         nn.Linear(self.channels, self.channels)\n","        #     ))\n","        self.attn_blocks = nn.ModuleList([AttnBlock1(in_channels, self.num_heads) for i in range(x)])\n","        self.LN = nn.LayerNorm(self.channels)\n","        self.onexone2 = nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1)\n","        \n","    \n","    def forward(self, x, slots):\n","        x = self.norm1(x)\n","        x = self.onexone(x)\n","        b, c, h, w = x.shape\n","        x = x.permute(0, 2, 3, 1)\n","        x = x.reshape(b, h*w, c)\n","#         print(slots.shape)\n","        for block in self.attn_blocks:\n","            x = block(x, slots)\n","        x = x.reshape(b, h, w, c)\n","        x = x.permute(0, 3, 1, 2)\n","        x = self.onexone2(x)\n","        return x\n","            \n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.213656Z","iopub.status.busy":"2024-05-07T15:58:07.213429Z","iopub.status.idle":"2024-05-07T15:58:07.231847Z","shell.execute_reply":"2024-05-07T15:58:07.231130Z","shell.execute_reply.started":"2024-05-07T15:58:07.213636Z"},"trusted":true},"outputs":[],"source":["class TimeEmbedding(nn.Module):\n","    def __init__(self, emb_channels):\n","        super(TimeEmbedding, self).__init__()\n","        self.emb_channels = emb_channels\n","        \n","    def forward(self, t):\n","        # dimension of t = (batch_size, 1)\n","        t = t.float()\n","        t = t.repeat(1, self.emb_channels)\n","        temp = torch.randn_like(t)\n","        for i in range(self.emb_channels):\n","            t[:, i] = t[:, i] / 10000.0**(2*i/self.emb_channels)\n","#             print(\"hahahhhahhaaaaa\" ,i, t[:, i])\n","            if(i%2 == 0):\n","                temp[:, i] = torch.sin(t[:, i//2])\n","            else:\n","                temp[:, i] = torch.cos(t[:, i//2])\n","#                 print(\"ahhoaw\",i//2,t[:, i//2])\n","\n","        return temp\n","            \n","\n"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.235623Z","iopub.status.busy":"2024-05-07T15:58:07.235361Z","iopub.status.idle":"2024-05-07T15:58:07.267492Z","shell.execute_reply":"2024-05-07T15:58:07.266767Z","shell.execute_reply.started":"2024-05-07T15:58:07.235601Z"},"trusted":true},"outputs":[],"source":["class UNET(nn.Module):\n","    def __init__(self, embedding_dim, img_size, transformer_iter, C=64):\n","        super(UNET, self).__init__()\n","        self.C = C\n","        self.embedding_dim = embedding_dim\n","        self.img_size = img_size\n","        self.transformer_iter = transformer_iter\n","\n","        # ??\n","        self.conv_in = nn.Conv2d(in_channels=3, out_channels=C, kernel_size=3, padding=1, stride=1)\n","        self.R1 = RESBLOCK(C, C, embedding_dim)\n","        self.R2 = RESBLOCK(C, C, embedding_dim)\n","        self.D1 = RESBLOCK(C, C, embedding_dim, type_='downsample')\n","        self.R3 = RESBLOCK(C, 2*C, embedding_dim)\n","        self.T1 = AttentionBlock(2*C, transformer_iter)\n","        self.R4 = RESBLOCK(2*C, 2*C, embedding_dim)\n","        self.T2 = AttentionBlock(2*C, transformer_iter)\n","        self.D2 = RESBLOCK(2*C, 2*C, embedding_dim, type_='downsample')\n","        self.R5 = RESBLOCK(2*C, 3*C, embedding_dim)\n","        self.T3 = AttentionBlock(3*C, transformer_iter)\n","        self.R6 = RESBLOCK(3*C, 3*C, embedding_dim)\n","        self.T4 = AttentionBlock(3*C, transformer_iter)\n","        self.D3 = RESBLOCK(3*C, 3*C, embedding_dim, type_='downsample')\n","        self.R7 = RESBLOCK(3*C, 4*C, embedding_dim)\n","        self.T5 = AttentionBlock(4*C, transformer_iter)\n","        self.R8 = RESBLOCK(4*C, 4*C, embedding_dim)\n","        self.T6 = AttentionBlock(4*C, transformer_iter)\n","\n","        self.R9 = RESBLOCK(4*C, 4*C, embedding_dim)\n","        self.T7 = AttentionBlock(4*C, transformer_iter)\n","        self.R10 = RESBLOCK(4*C, 4*C, embedding_dim)\n","\n","        self.R11 = RESBLOCK(8*C, 4*C, embedding_dim)\n","        self.T8 = AttentionBlock(4*C, transformer_iter)\n","        self.R12 = RESBLOCK(8*C, 4*C, embedding_dim)\n","        self.T9 = AttentionBlock(4*C, transformer_iter)\n","        self.R13 = RESBLOCK(7*C, 4*C, embedding_dim)\n","        self.T10 = AttentionBlock(4*C, transformer_iter)\n","        self.U1 = RESBLOCK(4*C, 4*C, embedding_dim, type_='upsample')\n","        self.R14 = RESBLOCK(7*C, 3*C, embedding_dim)\n","        self.T11 = AttentionBlock(3*C, transformer_iter)\n","        self.R15 = RESBLOCK(6*C, 3*C, embedding_dim)\n","        self.T12 = AttentionBlock(3*C, transformer_iter)\n","        self.R16 = RESBLOCK(5*C, 3*C, embedding_dim)\n","        self.T13 = AttentionBlock(3*C, transformer_iter)\n","        self.U2 = RESBLOCK(3*C, 3*C, embedding_dim, type_='upsample')\n","        self.R17 = RESBLOCK(5*C, 2*C, embedding_dim)\n","        self.T14 = AttentionBlock(2*C, transformer_iter)\n","        self.R18 = RESBLOCK(4*C, 2*C, embedding_dim)\n","        self.T15 = AttentionBlock(2*C, transformer_iter)\n","        self.R19 = RESBLOCK(3*C, 2*C, embedding_dim)\n","        self.T16 = AttentionBlock(2*C, transformer_iter)\n","        self.U3 = RESBLOCK(2*C, 2*C, embedding_dim, type_='upsample')\n","        self.R20 = RESBLOCK(3*C, C, embedding_dim)\n","        self.R21 = RESBLOCK(2*C, C, embedding_dim)\n","        self.R22 = RESBLOCK(2*C, C, embedding_dim)\n","        self.norm = nn.GroupNorm(32, C)\n","        # ??\n","        self.conv_op = nn.Conv2d(in_channels=C, out_channels=3, kernel_size=3, padding=1, stride=1)\n","\n","    def forward(self, x, time_embedding, slots):\n","        x = self.conv_in(x)\n","        conv_in = x\n","        x = self.R1(x, time_embedding)\n","        r1 = x\n","        x = self.R2(x, time_embedding)\n","        r2 = x\n","        x = self.D1(x, time_embedding)\n","        d1 = x\n","        x = self.R3(x, time_embedding)\n","        x = self.T1(x, slots)\n","        t1 = x\n","        x = self.R4(x, time_embedding)\n","        x = self.T2(x, slots)\n","        t2 = x\n","        x = self.D2(x, time_embedding)\n","        d2 = x\n","        x = self.R5(x, time_embedding)\n","        x = self.T3(x, slots)\n","        t3 = x\n","        x = self.R6(x, time_embedding)\n","        x = self.T4(x, slots)\n","        t4 = x\n","        x = self.D3(x, time_embedding)\n","        d3 = x\n","        x = self.R7(x, time_embedding)\n","        x = self.T5(x, slots)\n","        t5 = x\n","        x = self.R8(x, time_embedding)\n","        x = self.T6(x, slots)\n","        t6 = x\n","\n","        x = self.R9(x, time_embedding)\n","        x = self.T7(x, slots)\n","        x = self.R10(x, time_embedding)\n","\n","        x = torch.cat([x, t6], dim=1)\n","        x = self.R11(x, time_embedding)\n","        x = self.T8(x, slots)\n","        x = torch.cat([x, t5], dim=1)\n","        x = self.R12(x, time_embedding)\n","        x = self.T9(x, slots)\n","        x = torch.cat([x, d3], dim=1)\n","        x = self.R13(x, time_embedding)\n","        x = self.T10(x, slots)\n","        x = self.U1(x, time_embedding)\n","        x = torch.cat([x, t4], dim=1)\n","        x = self.R14(x, time_embedding)\n","        x = self.T11(x, slots)\n","        x = torch.cat([x, t3], dim=1)\n","        x = self.R15(x, time_embedding)\n","        x = self.T12(x, slots)\n","        x = torch.cat([x, d2], dim=1)\n","        x = self.R16(x, time_embedding)\n","        x = self.T13(x, slots)\n","        x = self.U2(x, time_embedding)\n","        x = torch.cat([x, t2], dim=1)\n","        x = self.R17(x, time_embedding)\n","        x = self.T14(x, slots)\n","        x = torch.cat([x, t1], dim=1)\n","        x = self.R18(x, time_embedding)\n","        x = self.T15(x, slots)\n","        x = torch.cat([x, d1], dim=1)\n","        x = self.R19(x, time_embedding)\n","        x = self.T16(x, slots)\n","        x = self.U3(x, time_embedding)\n","        x = torch.cat([x, r2], dim=1)\n","        x = self.R20(x, time_embedding)\n","        x = torch.cat([x, r1], dim=1)\n","        x = self.R21(x, time_embedding)\n","        x = torch.cat([x, conv_in], dim=1)\n","        x = self.R22(x, time_embedding)\n","        x = self.norm(x)\n","        x = self.conv_op(x)\n","        \n","        return x\n","        \n"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.268819Z","iopub.status.busy":"2024-05-07T15:58:07.268564Z","iopub.status.idle":"2024-05-07T15:58:07.289584Z","shell.execute_reply":"2024-05-07T15:58:07.288721Z","shell.execute_reply.started":"2024-05-07T15:58:07.268796Z"},"trusted":true},"outputs":[],"source":["class LDM(nn.Module):\n","    def __init__(self, vae):\n","        super(LDM, self).__init__()\n","        self.vae = vae\n","        \n","        self.unet = UNET(128, 32, 6)\n","        self.time_embedding_func = TimeEmbedding(128)\n","        self.encoder = CNNEncoder(64)\n","        self.sa = SlotAttention(11, 64, 3, 5, 64, 64, 64)\n","\n","        self.beta1 = 1e-4\n","        self.betaT = 2e-2\n","        self.alphas = torch.ones(1000)\n","        for i in range(1000):\n","            self.alphas[i] = self.alpha(i+1)\n","    def alpha(self, time_step):\n","        alphabar = 1.0\n","        for i in range(time_step):\n","            betat = self.beta1 + (self.betaT - self.beta1)*i/1000.0\n","            alphabar = alphabar * (1-betat)\n","        return alphabar\n","    def get_alphas(self, time):\n","        alphas = torch.randn_like(time.float())\n","        for i in range(time.size(0)):\n","            alphas[i][0] = self.alphas[time[i][0].item()-1]\n","        return alphas\n","    def get_xt_from_x0(self, x0 ,t, noise):\n","        alpha = self.get_alphas(t).unsqueeze(-1).unsqueeze(-1)\n","        mean = torch.sqrt(alpha) * x0\n","        std = torch.sqrt(1-alpha) * noise\n","        return mean + std\n","    def forward(self, x, noise):\n","        x_enc = self.encoder(x)\n","        slots,masks = self.sa(x_enc)\n","        with torch.no_grad():\n","            x = self.vae.encode(x)\n","        b = x.size(0)\n","        time_emb = torch.randint(1, 1001, (b, 1)).to(device)\n","#         print(\".........\",time_emb)\n","        xt = self.get_xt_from_x0(x, time_emb, noise)\n","        time_emb = self.time_embedding_func(time_emb)\n","#         print(time_emb)\n","#         print(\"time: \", time_emb)\n","        if np.isnan(xt[0][0][0][0].item()):\n","            print(\"NAN xt\")\n","        x = self.unet(xt, time_emb, slots)\n","        if np.isnan(x[0][0][0][0].item()):\n","            print(\"NAN\")\n","        return x\n","    def denoise(self, xprev, noise, t):\n","        alphas = self.alphas[t-1]\n","        beta = self.beta1 + (self.betaT - self.beta1)*t/1000.0\n","        z = torch.randn_like(xprev)\n","        if  t != 1:\n","            x = (1/np.sqrt((1-beta)))*(xprev - (beta*noise)/np.sqrt(1-alphas)) + np.sqrt(beta)*z\n","        else:\n","            x = (1/np.sqrt((1-beta)))*(xprev - (beta*noise)/np.sqrt(1-alphas)) \n","        return x\n","\n","    def infer(self, x):\n","        x_enc = self.encoder(x)\n","        slots, masks = self.sa(x_enc)\n","#         return masks\n","        with torch.no_grad():\n","            x = self.vae.encode(x)\n","        b = x.size(0)\n","        noise = torch.randn_like(x)\n","        xprev = noise\n","        for t in range(1000, 0, -1):\n","            time_emb = torch.tensor(t).to(device)\n","            time_emb = self.time_embedding_func(time_emb)\n","            x = self.unet(xprev, time_emb, slots)\n","            xprev = self.denoise(xprev, x, t)\n","        return self.vae.decode(xprev)"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.290761Z","iopub.status.busy":"2024-05-07T15:58:07.290496Z","iopub.status.idle":"2024-05-07T15:58:07.304876Z","shell.execute_reply":"2024-05-07T15:58:07.304149Z","shell.execute_reply.started":"2024-05-07T15:58:07.290738Z"},"trusted":true},"outputs":[],"source":["class HDF5Dataset(Dataset):\n","    def __init__(self, hdf5_file):\n","        self.hdf5_file = hdf5_file\n","        \n","        # Open HDF5 file\n","        self.hdf5_handle = h5py.File(hdf5_file, 'r')\n","        self.data = self.hdf5_handle['images']\n","        self.images = [(2 * (torch.tensor(self.data[i], dtype=torch.float32) / 255) - 1) for i in range(len(self.data))]\n","#         self.masks = self.hdf5_handle['masks']\n","        \n","    def __len__(self):\n","        return len(self.images)\n","    \n","    def __getitem__(self, idx):\n","        return self.images[idx]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-05-07T15:58:07.306142Z","iopub.status.busy":"2024-05-07T15:58:07.305826Z"},"trusted":true},"outputs":[],"source":["train_dataset = HDF5Dataset(dir + 'train_dataset.h5')\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["val_dataset = HDF5Dataset(dir + 'val_dataset.h5')\n","val_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vae = VAE()\n","ckpt = torch.load('/kaggle/input/vae/pytorch/1/1/vae_checkpoint.pth')\n","vae.load_state_dict(ckpt)\n","SDM = LDM(vae)\n","SDM = nn.DataParallel(SDM)\n","# SDM.load_state_dict(torch.load('/kaggle/input/gg/pytorch/h/1/checkpoint.pth'))\n","# SDM = nn.DataParallel(SDM)\n","SDM.to(device)\n","optimizer = optim.Adam(SDM.parameters(), lr=2e-4)\n","criterion = nn.MSELoss()\n","n_epochs = 100\n","\n","init_lr = 0.0004\n","\n","warmup_iters = 10000\n","decay_steps = 100000\n","decay_rate = 0.5"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# counter = 0.0\n","# for epoch in range(n_epochs):\n","#     running_loss = 0.0\n","#     for i, data in enumerate(train_loader):\n","#         data = data.to(device)\n","#         noise = torch.randn(data.size(0), 3, 32, 32).to(device)\n","#         counter += 1\n","#         if counter < warmup_iters:\n","#                 learning_rate = init_lr*(counter/warmup_iters)\n","#         else:\n","#             learning_rate = init_lr\n","#         learning_rate = learning_rate * (decay_rate ** (counter/decay_steps))\n","#         optimizer.param_groups[0]['lr'] = learning_rate\n","#         optimizer.zero_grad()\n","#         output = SDM(data, noise)\n","#         loss = criterion(output, noise)\n","# #         print(output)\n","# #         print(noise)\n","# #         print(loss.item())\n","#         loss.backward()\n","#         torch.nn.utils.clip_grad_norm_(SDM.parameters(), 1.0)\n","#         optimizer.step()\n","#         running_loss += loss.item()\n","# #         break\n","#         if i % 10 == 9:\n","# #             break\n","#             print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","#             running_loss = 0.0\n","#         del loss, output, noise, data\n","#     torch.save(SDM.state_dict(), os.path.join(out_dir, f'checkpoint.pth'))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def adjusted_rand_index(true_ids, pred_ids):\n","    \"\"\"\n","    Args:\n","        true_masks: Integer ids for objects\n","            [batch_size, H, W].  \n","            as integer ids.\n","        pred_masks: An integer-valued array of shape\n","            [batch_size, K, H, W]. The predicted cluster assignment\n","            encoded as integer ids.\n","        ignore_background: Boolean, if True, then ignore all pixels where\n","            true_ids == 0 (default: False).\n","\n","    Returns:\n","        ARI scores as a float32 array of shape [batch_size].\n","    \"\"\"\n","    pred_ids = pred_ids.argmax(dim=-3)  # [B, N, H, W] --> [B, H, W]\n","\n","    if len(true_ids.shape) == 3:\n","        true_ids = true_ids.unsqueeze(1)\n","    if len(pred_ids.shape) == 3:\n","        pred_ids = pred_ids.unsqueeze(1)\n","\n","    true_oh = F.one_hot(true_ids).float()\n","    pred_oh = F.one_hot(pred_ids).float()\n","\n","    N = torch.einsum(\"bthwc,bthwk->bck\", true_oh, pred_oh)\n","    A = torch.sum(N, dim=-1)  # row-sum  (batch_size, c)\n","    B = torch.sum(N, dim=-2)  # col-sum  (batch_size, k)\n","    num_points = torch.sum(A, dim=1)\n","\n","    rindex = torch.sum(N * (N - 1), dim=[1, 2])\n","    aindex = torch.sum(A * (A - 1), dim=1)\n","    bindex = torch.sum(B * (B - 1), dim=1)\n","    expected_rindex = aindex * bindex / torch.clamp(\n","        num_points * (num_points - 1), min=1)\n","    max_rindex = (aindex + bindex) / 2\n","    denominator = max_rindex - expected_rindex\n","    ari = (rindex - expected_rindex) / denominator\n","\n","    # There are two cases for which the denominator can be zero:\n","    # 1. If both label_pred and label_true assign all pixels to a single cluster.\n","    #    (max_rindex == expected_rindex == rindex == num_points * (num_points-1))\n","    # 2. If both label_pred and label_true assign max 1 point to each cluster.\n","    #    (max_rindex == expected_rindex == rindex == 0)\n","    # In both cases, we want the ARI score to be 1.0:\n","    return torch.where(denominator != 0, ari, torch.tensor(1.).type_as(ari))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["vae = VAE()\n","ckpt = torch.load('/kaggle/input/vae/pytorch/1/1/vae_checkpoint.pth')\n","vae.load_state_dict(ckpt)\n","SDM = LDM(vae)\n","SDM.load_state_dict(torch.load('/kaggle/input/gg/pytorch/h/1/checkpoint.pth'))\n","SDM.eval()\n","SDM.to(device)\n","criterion = nn.MSELoss()\n","running_loss = 0.0\n","ARI = 0\n","with torch.no_grad():\n","    for i, (data, true_masks) in enumerate(val_loader):\n","        data = data.to(device)\n","#         img = SDM.infer(data)\n","        pred_masks = SDM.infer(data)\n","        if ARI == 0:\n","            ARI = adjusted_rand_index(true_masks.long(), pred_masks.cpu())\n","        else:\n","            ARI += adjusted_rand_index(true_masks.long(), pred_masks.cpu())\n","        \n","        \n","        # img size = (1, 3, 128, 128) save the image\n","#         img = img.squeeze(0).detach().cpu().numpy().transpose(1, 2, 0)\n","#         img = (img + 1) / 2\n","#         img = img * 255\n","#         img = img.astype(np.uint8)\n","#         img = Image.fromarray(img)\n","#         img.save(os.path.join(out_dir, f'{i+4}.png'))\n","#         data = data.squeeze(0).detach().cpu().numpy().transpose(1, 2, 0)\n","#         data = (data + 1) / 2\n","#         data = data * 255\n","#         data = data.astype(np.uint8)\n","#         data = Image.fromarray(data)\n","#         data.save(os.path.join(out_dir, f'{i+4}_data.png'))\n","#         if i == 10:\n","#             break\n","\n","print(ARI/len(val_loader))\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4808627,"sourceId":8347590,"sourceType":"datasetVersion"},{"modelInstanceId":37334,"sourceId":44441,"sourceType":"modelInstanceVersion"},{"modelInstanceId":37341,"sourceId":44448,"sourceType":"modelInstanceVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
