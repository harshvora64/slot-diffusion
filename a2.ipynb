{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import h5py\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_epochs = 1\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "dir = 'A2_dataset/'\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDF5Dataset(Dataset):\n",
    "    def __init__(self, hdf5_file):\n",
    "        self.hdf5_file = hdf5_file\n",
    "        \n",
    "        # Open HDF5 file\n",
    "        self.hdf5_handle = h5py.File(hdf5_file, 'r')\n",
    "        self.images = self.hdf5_handle['images']\n",
    "        self.masks = self.hdf5_handle['masks']\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.tensor(self.images[idx], dtype=torch.float32), torch.tensor(self.masks[idx], dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HDF5Dataset(dir + 'train_dataset.h5')\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_dataset = HDF5Dataset(dir + 'val_dataset.h5')\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttention(nn.Module):\n",
    "    def __init__(self, k, d_common=64, n_iter_train=3,n_iter_test=5, d_slot=64, d_inputs=64, hid_dim=128):\n",
    "        super(SlotAttention, self).__init__()\n",
    "        self.k = k\n",
    "        self.d_common = d_common\n",
    "        self.n_iter_train = n_iter_train\n",
    "        self.n_iter_test = n_iter_test\n",
    "        self.d_slot = d_slot\n",
    "        self.d_inputs = d_inputs\n",
    "\n",
    "        self.fc_q = nn.Linear(d_slot, d_common)\n",
    "        self.fc_k = nn.Linear(d_inputs, d_common)\n",
    "        self.fc_v = nn.Linear(d_inputs, d_common)\n",
    "\n",
    "        self.gru = nn.GRUCell(d_common, d_slot)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_slot, hid_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hid_dim, d_slot)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.mu = nn.Parameter(torch.randn(1, 1,d_common))\n",
    "        self.sigma = nn.Parameter(torch.rand(1,1, d_common))\n",
    "\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch_size, n_inputs, d_inputs)\n",
    "        # slots: (batch_size, n_slots, d_slot)\n",
    "        if self.training:\n",
    "            n_iter = self.n_iter_train\n",
    "        else:\n",
    "            n_iter = self.n_iter_test\n",
    "        batch_size, n_inputs, d_inputs = inputs.size()\n",
    "        mu = self.mu.expand(batch_size, self.k, -1)\n",
    "        sigma = self.sigma.expand(batch_size, self.k, -1)\n",
    "        slots = torch.normal(mu, sigma).to(device)\n",
    "        inputs = nn.LayerNorm(d_inputs).to(device)(inputs)\n",
    "        k = self.fc_k(inputs)               # (batch_size, n_inputs, d_common)\n",
    "        v = self.fc_v(inputs)               # (batch_size, n_inputs, d_common)\n",
    "        for i in range(n_iter):\n",
    "            q = self.fc_q(nn.LayerNorm(self.d_slot).to(device)(slots))                # (batch_size, n_slots, d_common)\n",
    "\n",
    "            attn = torch.bmm(k, q.permute(0, 2, 1)) / np.sqrt(self.d_common)            # (batch_size, n_inputs, n_slots)\n",
    "            attn = self.softmax(attn) +  1e-8                                           # (batch_size, n_inputs, n_slots)\n",
    "            attn = attn / attn.sum(dim=1, keepdim=True)                                 # (batch_size, n_inputs, n_slots)\n",
    "            attn = attn.permute(0,2,1)\n",
    "            updates = torch.einsum('bjd,bij->bid', v, attn)                             # (batch_size, n_slots, d_common)\n",
    "\n",
    "\n",
    "            slots = self.gru(updates.reshape(-1,self.d_common), slots.reshape(-1, self.d_slot)).reshape(batch_size, self.k, self.d_slot)\n",
    "            slots = nn.LayerNorm(self.d_slot).to(device)(slots)\n",
    "            slots = slots + self.mlp(slots)\n",
    "        \n",
    "        return slots\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbeddings(nn.Module):\n",
    "    def __init__(self, H, W, hid_dim=64):\n",
    "        super(PositionalEmbeddings, self).__init__()\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "        self.hid_dim = hid_dim\n",
    "        self.project = nn.Linear(4, hid_dim)\n",
    "    \n",
    "    def construct_grid(self, H, W):\n",
    "        x = torch.linspace(0, 1, W).unsqueeze(0).repeat(H, 1)\n",
    "        y = torch.linspace(0, 1, H).unsqueeze(1).repeat(1, W)\n",
    "        return torch.stack([x, 1-x, y, 1-y], dim=2)    # (H, W, 4)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        grid = self.construct_grid(self.H, self.W).to(device)  # (H, W, 4)\n",
    "        grid = self.project(grid)\n",
    "        return inputs + grid.unsqueeze(0).expand(inputs.size(0), self.H, self.W, self.hid_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, hid_dim=64):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, hid_dim, 5, padding=2)                    \n",
    "        self.conv2 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n",
    "        self.conv3 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n",
    "        self.conv4 = nn.Conv2d(hid_dim, hid_dim, 5, padding=2)\n",
    "\n",
    "        self.positionalEmb = PositionalEmbeddings(128, 128, hid_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(hid_dim, hid_dim)\n",
    "        self.fc2 = nn.Linear(hid_dim, hid_dim)  \n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.conv1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv2(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv3(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.conv4(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        \n",
    "        inputs = self.positionalEmb(inputs.permute(0, 2, 3, 1))\n",
    "        inputs = inputs.flatten(1, 2)\n",
    "        inputs = nn.LayerNorm(inputs.size()[1:])(inputs) \n",
    "        inputs = self.fc1(inputs)\n",
    "        inputs = self.relu(inputs)\n",
    "        inputs = self.fc2(inputs)\n",
    "        return inputs\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deconvDecoder(nn.Module):\n",
    "    def __init__(self, hid_dim=64):\n",
    "        super(deconvDecoder, self).__init__()\n",
    "        self.hid_dim = hid_dim\n",
    "        self.deconv1 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, padding=2, output_padding=1, stride=2)\n",
    "        self.deconv2 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, padding=2, output_padding=1, stride=2)\n",
    "        self.deconv3 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, padding=2, output_padding=1, stride=2)\n",
    "        self.deconv4 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, padding=2, output_padding=1, stride=2)\n",
    "        self.deconv5 = nn.ConvTranspose2d(hid_dim, hid_dim, 5, padding=2, output_padding=0, stride=1)\n",
    "        self.deconv6 = nn.ConvTranspose2d(hid_dim, 4, 3, padding=1, output_padding=0, stride=1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.positionalEmb = PositionalEmbeddings(8, 8, hid_dim)\n",
    "\n",
    "    def forward(self, slots):\n",
    "        # slots: (batch_size, n_slots, d_slot)\n",
    "        b, k, d = slots.size()\n",
    "        slots = slots.unsqueeze(2).unsqueeze(3).expand(b, k, 8, 8, d)\n",
    "        slots = slots.reshape(b*k, 8, 8, d)\n",
    "        slots = self.positionalEmb(slots)\n",
    "        slots = slots.permute(0, 3, 1, 2)\n",
    "        slots = self.deconv1(slots)\n",
    "        slots = self.relu(slots)\n",
    "        slots = self.deconv2(slots)\n",
    "        slots = self.relu(slots)\n",
    "        slots = self.deconv3(slots)\n",
    "        slots = self.relu(slots)\n",
    "        slots = self.deconv4(slots)\n",
    "        slots = self.relu(slots)\n",
    "        slots = self.deconv5(slots)\n",
    "        slots = self.relu(slots)\n",
    "        slots = self.deconv6(slots)                 # (batch_size * n_slots, 4, 128, 128)\n",
    "\n",
    "        slots = slots.reshape(b, k, 4, 128, 128)\n",
    "        slots = slots.permute(0, 1, 3, 4, 2)\n",
    "        contents, masks = slots.split([3, 1], dim=4)        # (batch_size, n_slots, 128, 128, 3)\n",
    "        masks = nn.Softmax(dim=1)(masks)                    # (batch_size, n_slots, 128, 128, 1)\n",
    "        img = (contents * masks).sum(dim=1)\n",
    "        img = img.permute(0, 3, 1, 2)\n",
    "\n",
    "        return img\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlotAttentionModel(nn.Module):\n",
    "    def __init__(self, k, d_common=64, n_iter_train=3, n_iter_test=5, d_slot=64, d_inputs=64, hid_dim=64):\n",
    "        super(SlotAttentionModel, self).__init__()\n",
    "        self.encoder = CNNEncoder(hid_dim)\n",
    "        self.slotAttention = SlotAttention(k, d_common, n_iter_train, n_iter_test, d_slot, d_inputs, hid_dim)\n",
    "        self.decoder = deconvDecoder(hid_dim)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        features = self.encoder(inputs)\n",
    "        slots = self.slotAttention(features)\n",
    "        img = self.decoder(slots)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "SAM = SlotAttentionModel(4, 64, 3, 5, 64, 64, 64).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "init_lr = 0.0004\n",
    "optimizer = optim.Adam(SAM.parameters(), lr=init_lr)\n",
    "\n",
    "\n",
    "warmup_iters = 10000\n",
    "decay_steps = 10000\n",
    "decay_rate = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "def train(model, criterion, optimizer, train_loader, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, (images, masks) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            if i < warmup_iters:\n",
    "                learning_rate = init_lr*(i/warmup_iters)\n",
    "            else:\n",
    "                learning_rate = init_lr\n",
    "            learning_rate = learning_rate * (decay_rate ** (i/decay_steps))\n",
    "            optimizer.param_groups[0]['lr'] = learning_rate\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if i % 10 == 0:\n",
    "                print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n",
    "                running_loss = 0.0\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 2480.615\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(SAM, criterion, optimizer, train_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
